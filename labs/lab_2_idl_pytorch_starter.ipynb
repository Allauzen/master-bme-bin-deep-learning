{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a simple introduction to pytorch, assuming you already know  python, numpy and the notebooks. \n",
    "\n",
    "\n",
    "To start with pytorch, here are some external websites: \n",
    "* http://pytorch.org/tutorials/ : official tutorials\n",
    "* http://pytorch.org/docs/master/ : official documentation\n",
    "\n",
    "Before, check the version of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "print(th.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have a version of at least 1.0.0. \n",
    "\n",
    "\n",
    "## Tensor  overview\n",
    "For users who are familiar with numpy arrays, the PyTorch **Tensor** class is very similar. PyTorch is like NumPy, but with GPU acceleration and automatic computation of gradients. This  makes it suitable for deep learning: calculating backward pass data automatically starting from a forward expression.\n",
    "\n",
    "The forward pass is implemented as a computation graph. The **Tensor** is the basic piece of this computation graph, to encode the data (input/output) and the parameters of the model. \n",
    "A Tensor is both a tensor (like a numpy array or a matlab matrix) and a variable (or a node) of the computation graph. A Tensor can store data and the associated gradients.\n",
    "\n",
    "\n",
    "\n",
    "**IMPORTANT NOTE: ** Since torch 0.3, a **Tensor** is a **Variable** that wraps a tensor. Before these 2 concepts were separated. \n",
    "\n",
    "## Module overview\n",
    "\n",
    "A module is a part of a NNet. It may contains Tensors. The core PyTorch modules for building neural networks are located in *torch.nn*, which provides common neural network layers and other architectural components. Fully connected layers, convolutional layers, activation functions, and loss functions can all be found here. Modules can be seen as pre-built pieces of computation graph. \n",
    "\n",
    "A simple example of *module* is **Linear**: it's a fully connected layer, so a linear transformation of the input. It contains a matrix of parameters (a Tensor). Activation function are also *module*. You can therefore create a cascade of *Linear* module with a *Sigmoid*, for example. \n",
    "\n",
    "A special kind of module is a *container* : a module that contains other module. The most widely used is **Sequential**: it's a container to implement a feed-forward network. When you create a **Sequential** object you pass him an ordered list of modules to create the cascade of operation. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tensor\n",
    "\n",
    "To start with  *Tensor*s, read this link first :\n",
    "http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html. \n",
    "and then look at the operations on tensors:  http://pytorch.org/docs/master/torch.html. \n",
    "\n",
    "## Basics\n",
    "You should know how to : \n",
    "* Build a tensor of dimensions (2,3) filled with integers from 1  to 6. \n",
    "* Convert this  Tensor in array numpy and back. \n",
    "* Compute the sum of its elements, the sum per rows and per columns. \n",
    "* Build a tensor of dimensions (3,2) filled with random numbers. Numbers are drawn from the uniform distribution on [0,1]\n",
    "* Same with a gaussian  (mean=0, variance=1). \n",
    "\n",
    "Your turn to code but remember: \n",
    "- you can use auto-completion\n",
    "- ask for help, like this : \n",
    "But in most of the case it is easier to use the online documentation of the function: https://pytorch.org/docs/stable/torch.html#torch.arange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function arange:\n",
      "\n",
      "arange(...)\n",
      "    arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "    \n",
      "    Returns a 1-D tensor of size :math:`\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil`\n",
      "    with values from the interval ``[start, end)`` taken with common difference\n",
      "    :attr:`step` beginning from `start`.\n",
      "    \n",
      "    Note that non-integer :attr:`step` is subject to floating point rounding errors when\n",
      "    comparing against :attr:`end`; to avoid inconsistency, we advise adding a small epsilon to :attr:`end`\n",
      "    in such cases.\n",
      "    \n",
      "    .. math::\n",
      "        \\text{out}_{{i+1}} = \\text{out}_{i} + \\text{step}\n",
      "    \n",
      "    Args:\n",
      "        start (Number): the starting value for the set of points. Default: ``0``.\n",
      "        end (Number): the ending value for the set of points\n",
      "        step (Number): the gap between each pair of adjacent points. Default: ``1``.\n",
      "        out (Tensor, optional): the output tensor\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`). If `dtype` is not given, infer the data type from the other input\n",
      "            arguments. If any of `start`, `end`, or `stop` are floating-point, the\n",
      "            `dtype` is inferred to be the default dtype, see\n",
      "            :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to\n",
      "            be `torch.int64`.\n",
      "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "            Default: ``torch.strided``.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if ``None``, uses the current device for the default tensor type\n",
      "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.arange(5)\n",
      "        tensor([ 0,  1,  2,  3,  4])\n",
      "        >>> torch.arange(1, 4)\n",
      "        tensor([ 1,  2,  3])\n",
      "        >>> torch.arange(1, 2.5, 0.5)\n",
      "        tensor([ 1.0000,  1.5000,  2.0000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A hint \n",
    "help(th.arange)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation and access\n",
    "\n",
    "* Extract the first row and the last row (do the same with columns)\n",
    "* Build a matrix  A of dimension (2,3), a matrix  B (2,1) et and  C (1,4) with random initialisation. \n",
    "* Concatenate A with B, and the results with avec C. \n",
    "* Create A (5,4), then B (3,4) which contains in this order: the second, the first and the fourth row of A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the following code and how  x2 is built from x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3981, -1.0425,  0.1220,  1.2108],\n",
      "        [-0.0601,  0.1017, -0.1414,  0.7120],\n",
      "        [ 0.1204, -0.2236,  0.1505, -1.3086],\n",
      "        [ 0.3020, -0.1181,  1.7502, -0.7138],\n",
      "        [ 0.5890, -2.2146,  1.4481,  1.1310]])\n",
      "tensor([[-0.3981, -1.0425,  0.1220,  1.2108],\n",
      "        [-0.0601,  0.1017, -0.1414,  0.7120],\n",
      "        [ 0.1204, -0.2236,  0.1505, -1.3086],\n",
      "        [ 0.3020, -0.1181,  1.7502, -0.7138],\n",
      "        [ 0.5890, -2.2146,  1.4481,  1.1310]])\n",
      "torch.Size([2, 5, 4])\n",
      "tensor([[-0.3981, -1.0425,  0.1220,  1.2108],\n",
      "        [-0.3981, -1.0425,  0.1220,  1.2108]])\n",
      "torch.Size([5, 2, 4])\n",
      "tensor([[-0.3981, -0.3981],\n",
      "        [-1.0425, -1.0425],\n",
      "        [ 0.1220,  0.1220],\n",
      "        [ 1.2108,  1.2108]])\n",
      "torch.Size([5, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "x = th.randn(5, 4)\n",
    "print(x)\n",
    "\n",
    "x2= th.stack((x,x) , dim=0)\n",
    "print (x2[0]) \n",
    "print (x2.size()) \n",
    "\n",
    "x2= th.stack((x,x) , dim=1)\n",
    "print (x2[0]) \n",
    "print (x2.size()) \n",
    "\n",
    "x2= th.stack((x,x) , dim=2)\n",
    "print (x2[0]) \n",
    "print (x2.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape \n",
    "\n",
    "The method ** .view() ** is similar to *reshape*. This is **important** since with neural net, you will often need to play with dimensions. \n",
    "\n",
    "* Build a tensor of size (2, 3, 4)\n",
    "* Convert it in a matrix of dimension (3,8) and (2,12)\n",
    "* What does  *view(2,-1)*  do ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-grad\n",
    "\n",
    "`torch.autograd` provides classes and functions implementing automatic differentiation. \n",
    "When a tensor is created with `requires_grad=True`, the object will be able to store information about the gradient. \n",
    "In the following example, we build a computational graph. The \"end\" must be a scalar for automatic differentiation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0325,  0.6633],\n",
      "        [-0.7034,  0.4981]], requires_grad=True)\n",
      "tensor([[-0.0325,  0.6633],\n",
      "        [-0.7034,  0.4981]])\n",
      "tensor([[-0.0325,  0.6633],\n",
      "        [-0.7034,  0.4981]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = th.randn(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "out = 0.5*x.pow(2).sum() # out is a new variable (scalar)\n",
    "out.backward()       # back propagation in the graph\n",
    "print(x.grad)        # the gradient of out with respect to x \n",
    "print(x)             # A simple check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple dataset\n",
    "Let start with an easy dataset for binary classification. The following subsections just provide a dummy dataset and function to visualize the data-set. \n",
    "\n",
    "\n",
    "\n",
    "## Create the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ST1 = np.array([[17.0 ,12 ,13 ,15 ,15 ,20 ,20],[ 10 ,12 ,14 ,15 ,20 ,15 ,20]]) # class 1 \n",
    "ST2 = np.array([4, 7.5, 10 ,11, 5 ,5 ,6, 8, 5, 0, 5, 0, 10, 6]).reshape(2,7) # class 2 \n",
    "Xstudents = np.concatenate((ST1,ST2),axis=1)\n",
    "\n",
    "Ystudents = np.ones(14)\n",
    "Ystudents[7:] = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyTorch\n",
    "\n",
    "\n",
    "Define a logistic regression model with pytorch, learn it and vizualise the result. \n",
    "The roadmap is: \n",
    "* A simple neural model can rely on  **Sequential**. A model handles  **Tensors**. The data for a model should be converted into Tensors. Start by this transformation. \n",
    "* Create a regression model  (a single neuron with the logistic activation function, or a linear layer with one single neuron with the logistic activation). \n",
    "* Define the '**optimizer** (Take the basic Stochastic Gradient Descent). \n",
    "* Define the objective function\n",
    "* Write the training loop and run it until convergence. It can be useful to play with learning rate. Run the gradient descent example by example. \n",
    "* Look at the solution \n",
    "* Start again in  **batch** mode (the gradient is estimated on the whole training set).\n",
    "\n",
    "\n",
    "\n",
    "## From data to tensors / variables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17.0000, 10.0000],\n",
      "        [12.0000, 12.0000],\n",
      "        [13.0000, 14.0000],\n",
      "        [15.0000, 15.0000],\n",
      "        [15.0000, 20.0000],\n",
      "        [20.0000, 15.0000],\n",
      "        [20.0000, 20.0000],\n",
      "        [ 4.0000,  8.0000],\n",
      "        [ 7.5000,  5.0000],\n",
      "        [10.0000,  0.0000],\n",
      "        [11.0000,  5.0000],\n",
      "        [ 5.0000,  0.0000],\n",
      "        [ 5.0000, 10.0000],\n",
      "        [ 6.0000,  6.0000]])\n",
      "Y= tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.]) torch.Size([14])\n",
      "One i/O\n",
      "tensor([12., 12.]) torch.Size([2])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "X = th.FloatTensor(Xstudents.T)\n",
    "print(X)\n",
    "\n",
    "Y=th.FloatTensor(Ystudents)\n",
    "print(\"Y=\",Y,Y.shape)\n",
    "print (\"One i/O\")\n",
    "print(X[1],X[1].shape)\n",
    "print(Y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model, its loss and optimizer\n",
    "\n",
    "The model is a linear transformation followed by a Sigmoid function. This is equivalent to a logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (1): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# The model \n",
    "D_in=2  # input size : 2 \n",
    "D_out=1 # output size: one value \n",
    "model = th.nn.Sequential(\n",
    "    th.nn.Linear(D_in, D_out),\n",
    "    th.nn.Sigmoid()    \n",
    ")\n",
    "print(model)\n",
    "\n",
    "\n",
    "loss_fn = th.nn.BCELoss() # The binary cross entropy \n",
    "learning_rate = 1e-2\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=learning_rate) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the first input:  tensor([0.9987], grad_fn=<SigmoidBackward>)\n",
      "For the 3 first inputs:  tensor([[0.9987],\n",
      "        [0.9980],\n",
      "        [0.9991]], grad_fn=<SigmoidBackward>)\n",
      "For all:  tensor([[0.9987],\n",
      "        [0.9980],\n",
      "        [0.9991],\n",
      "        [0.9996],\n",
      "        [0.9999],\n",
      "        [0.9999],\n",
      "        [1.0000],\n",
      "        [0.9590],\n",
      "        [0.9517],\n",
      "        [0.8763],\n",
      "        [0.9773],\n",
      "        [0.6995],\n",
      "        [0.9821],\n",
      "        [0.9509]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# With a single input vector \n",
    "prediction = model(X[0]) # or prediction = model.forward(X[0]) both are equivalent\n",
    "\n",
    "print(\"For the first input: \",prediction)\n",
    "\n",
    "# With 3 input vectors \n",
    "prediction = model(X[0:3])\n",
    "print(\"For the 3 first inputs: \",prediction)\n",
    "\n",
    "# For the whole dataset\n",
    "prediction = model(X)\n",
    "print(\"For all: \",prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first prediction:  tensor([0.9987], grad_fn=<SigmoidBackward>) torch.Size([1])\n",
      "The reference:  tensor(1.) torch.Size([])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allauzen/anaconda/lib/python3.6/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0013, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With a single input vector \n",
    "prediction = model(X[0])\n",
    "print(\"The first prediction: \",prediction, prediction.shape)\n",
    "print(\"The reference: \",Y[0], Y[0].shape)\n",
    "\n",
    "loss_fn(prediction,Y[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code should generate a warning, since the label (or target value) and the prediction (considered as the input value of the loss) are of different dimensions. \n",
    "\n",
    "There is two ways to fix that. The first one is to reduce the input dimension using *squeeze*. The second one is to modify the target values. See the two cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first prediction:  tensor(0.9987, grad_fn=<SqueezeBackward0>) torch.Size([])\n",
      "The reference:  tensor(1.) torch.Size([])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0013, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(X[0]).squeeze()\n",
    "print(\"The first prediction: \",prediction, prediction.shape)\n",
    "print(\"The reference: \",Y[0], Y[0].shape)\n",
    "\n",
    "loss_fn(prediction,Y[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first prediction:  tensor([0.9987], grad_fn=<SigmoidBackward>) torch.Size([1])\n",
      "The reference:  tensor([1.]) torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0013, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(X[0])\n",
    "Ymodified = Y.view(-1,1)\n",
    "print(\"The first prediction: \",prediction, prediction.shape)\n",
    "print(\"The reference: \",Ymodified[0], Ymodified[0].shape)\n",
    "\n",
    "loss_fn(prediction,Ymodified[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Now we have everything to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(10.5804, grad_fn=<AddBackward0>)\n",
      "100 tensor(5.1808, grad_fn=<AddBackward0>)\n",
      "200 tensor(3.6035, grad_fn=<AddBackward0>)\n",
      "300 tensor(2.7501, grad_fn=<AddBackward0>)\n",
      "400 tensor(2.2338, grad_fn=<AddBackward0>)\n",
      "500 tensor(1.8887, grad_fn=<AddBackward0>)\n",
      "600 tensor(1.6409, grad_fn=<AddBackward0>)\n",
      "700 tensor(1.4537, grad_fn=<AddBackward0>)\n",
      "800 tensor(1.3069, grad_fn=<AddBackward0>)\n",
      "900 tensor(1.1885, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Nepochs = 1000 # number of epochs \n",
    "Nprint  = Nepochs/10  # frequence of print \n",
    "for epoch in range(Nepochs):\n",
    "    total=0.\n",
    "    for i in range(14):\n",
    "        prediction = model(X[i]).squeeze()    \n",
    "        loss = loss_fn(prediction, Y[i])\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        total+=loss\n",
    "    if epoch%Nprint==0:\n",
    "        print(epoch,total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the \"solution\" \n",
    "\n",
    "Here, we look at the different wrapping steps: \n",
    "- The model is a set of modules\n",
    "- A Linear module is a matrix of weights along with a bias vector. They are parameters.\n",
    "- A Parameter wrap a tensor\n",
    "- A tensor can be casted as a numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "mod = model[0]\n",
    "print(type(mod))\n",
    "print(type(mod.bias))\n",
    "print(type(mod.bias.data))\n",
    "print(type(mod.bias.data.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.1232]])\n",
      "tensor([[0.3244, 0.3608]])\n"
     ]
    }
   ],
   "source": [
    "print(mod.bias.data.view(1,1))\n",
    "print(mod.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Impact of the learning rate \n",
    "\n",
    "Now, we will use the same model trained with a different learning rate. The training process restarts from scratch. We need to therefore to re-create the model and the associated optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = th.nn.Sequential(\n",
    "    th.nn.Linear(D_in, D_out),\n",
    "    th.nn.Sigmoid()    \n",
    ")\n",
    "learning_rate = 1e-1\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the same model as before, randomly initialized. We train this same model with a different learning rate, a larger one. \n",
    "\n",
    "- Run the training with the same number of epochs and compare the loss value we get at the end\n",
    "- Do you think we can reach the same value with the learning rate of 1e-2, but with a longer training ? \n",
    "- Try the same thing with a learning rate of 0.5, what do you observe ? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(19.0010, grad_fn=<AddBackward0>)\n",
      "100 tensor(10.1366, grad_fn=<AddBackward0>)\n",
      "200 tensor(6.1699, grad_fn=<AddBackward0>)\n",
      "300 tensor(0.0167, grad_fn=<AddBackward0>)\n",
      "400 tensor(0.0139, grad_fn=<AddBackward0>)\n",
      "500 tensor(0.0136, grad_fn=<AddBackward0>)\n",
      "600 tensor(0.0135, grad_fn=<AddBackward0>)\n",
      "700 tensor(0.0134, grad_fn=<AddBackward0>)\n",
      "800 tensor(0.0133, grad_fn=<AddBackward0>)\n",
      "900 tensor(0.0132, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Nepochs = 1000 # number of epochs \n",
    "Nprint  = Nepochs/10  # frequence of print \n",
    "for epoch in range(Nepochs):\n",
    "    total=0.\n",
    "    for i in range(14):\n",
    "        prediction = model(X[i]).squeeze()    \n",
    "        loss = loss_fn(prediction, Y[i])\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        total+=loss\n",
    "    if epoch%Nprint==0:\n",
    "        print(epoch,total)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
