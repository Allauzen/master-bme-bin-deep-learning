{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab session in an introduction to feed-forward neural network with pytorch. We use the dataset Fashion-MNIST (see for more details this website https://github.com/zalandoresearch/fashion-mnist). The dataset contains 60000 and 10000 images for respectively training and testing. Each image is 28x28 pixels, for a total of 784 per image.  An image is presented to the neural network as a flat vector of 784 component. \n",
    "\n",
    "\n",
    "In this lab session, you will experiment different kind of feed-forward networks, starting with simple models,  and then increasing their complexity. \n",
    "\n",
    "First load and test python and pytorch. Your notebook is supposed to work with python 3 (see the top right corner of the notebook).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "print(th.__version__) # should be greater or equal to 1.0\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats=['svg']\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "To simplify, just download and / or read the picke file provided. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "fp = gzip.open('fashion-mnist.pk.gz','rb')\n",
    "allXtrain, allYtrain, Xtest, Ytest, classlist  = pickle.load(fp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: The dataset is split in two parts, the training set and the test set.\n",
    "For thorough study and evaluation of machine learning models, a good practice is to consider the data in 3 parts: \n",
    "- the **training** set to learn the model parameters;\n",
    "- the **validation** set to tune the hyper parameters and some design choices (the number and the size of the hidden layers, the dropout probability, ...);\n",
    "- the **test** set to evaluate the model at the end. \n",
    "\n",
    "\n",
    "For the moment, we leave the test set and focus on the training set. \n",
    "To spare time, we will only consider the first 20000 images for training in the following set of experiments. And we also build a validation set to compare the results we obtain with different hyper-parameters. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain, Ytrain  = allXtrain[:20000], allYtrain[:20000]\n",
    "Xvalid, Yvalid  = allXtrain[20000:30000], allYtrain[20000:30000]\n",
    "print(\"Training   shape:\" ,Xtrain.shape)\n",
    "print(\"Validation shape:\" ,Xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the dataset explore the training set made of Xtrain and Ytrain. \n",
    "- Look at the dimension and type of the tensors\n",
    "- Print also the classlist variable. \n",
    "- Then look at some example to check consistency. \n",
    "\n",
    "For that purpose you can plot an image like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(Xtrain[0].numpy().reshape(28,28) , matplotlib.pyplot.cm.gray)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A a first overview of the dataset, we can do a PCA (in 2D) of the training set. The following image represents the result: \n",
    "\n",
    "<img src=\"https://allauzen.github.io/assets/figs/pca-fashion-10-classes.png\" \n",
    "    style=\"width:400px; margin:0px auto;display:block\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward neural network\n",
    "\n",
    "\n",
    "\n",
    "A feedforward model can rely on the pytorch module *Sequential*. \n",
    "A *Sequential* module is a container of ordered modules: the first one takes input data and its output is given to feed the second module, and so on. \n",
    "\n",
    "**Note: ** In pytorch, modules assumed Tensors as input. The input Tensor can contain just one input (online mode) or several inputs (mini-batch). The first dimension of the input Tensor corresponds to the mini-batch, and the second one to the dimension of the example to feed in. For example, with a mini-batch of size B and an image of D pixels, the input Tensor should be of shape (B,D), event if B=1. \n",
    "\n",
    "\n",
    "## Shallow network\n",
    "\n",
    "Let start with a simple model with one input layer and one output layer (without hidden layers). Please refer to the examples provided previously, and propose an implementation of this linear model using the *Sequential* module as container.  To write the model, we must consider the fact that the model is trained in order to maximize the Log-likelihood on the training data. If you look at  https://pytorch.org/docs/stable/nn.html, the documentation of the NNet package of pytorch, there is a section on the loss functions. \n",
    "\n",
    "Two of the proposed loss function can be used for our purpose. The choice of one of them implies the choice of the activation function at the output layer. \n",
    "\n",
    "- What are these two possible choices ? \n",
    "\n",
    "\n",
    "Make your own choice and then fill the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_in = 784\n",
    "D_out= 10\n",
    "\n",
    "## \n",
    "model = nn.Sequential(\n",
    "        nn.Linear(D_in,D_out),\n",
    "        nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "loss_function = None \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then test the code on a minibatch of B examples. The code below corresponds to a prediction on a single image and then on 3 images. Look at the results, their shapes and values. Is it consistent with what you expect ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B=1\n",
    "i = 0\n",
    "pred = model(Xtrain[i:i+B])\n",
    "# explore \n",
    "print(pred)\n",
    "B=3\n",
    "i = 0\n",
    "pred = model(Xtrain[i:i+B])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B=1\n",
    "i = 1\n",
    "pred = model(Xtrain[i:i+B])\n",
    "loss = ??? \n",
    "# explore \n",
    "B=3\n",
    "i = 1\n",
    "pred = model(Xtrain[i:i+B])\n",
    "loss = ??? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online learning  and mini-batch\n",
    "\n",
    "We will use the Adam optimizer with an initial learning rate of 0.001. Read, modify the following code. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=30\n",
    "\n",
    "### The model, the loss and optimizer\n",
    "model=nn.Sequential(nn.Linear(D_in,D_out),\n",
    "                    nn.LogSoftmax(dim=1))\n",
    "optimizer=th.optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_fn=nn.NLLLoss()\n",
    "\n",
    "### Mini-batching and shuffle \n",
    "Ntrain = Xtrain.shape[0]\n",
    "Nvalid = Xvalid.shape[0]\n",
    "print(Ntrain,Nvalid)\n",
    "idx = np.arange(Ntrain)\n",
    "batch_size = 200\n",
    "nbatch = int(Ntrain/batch_size)\n",
    "print(batch_size, nbatch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_losses,valid_losses, valid_accuracies=[],[],[]\n",
    " \n",
    "\n",
    "for e in range(epochs):\n",
    "    np.random.shuffle(idx)\n",
    "    running_loss=0\n",
    "    totaln = 0\n",
    "    for bi in range(nbatch):\n",
    "        ids = idx[bi*batch_size:(bi+1)*batch_size]\n",
    "        images = Xtrain[ids]\n",
    "        labels = Ytrain[ids]\n",
    "        totaln += labels.shape[0] # the number of samples\n",
    "        optimizer.zero_grad()\n",
    "        logprobs=model(images)\n",
    "        loss=loss_fn(logprobs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "    # training is over for one epoch\n",
    "    # now compute interesting quantities\n",
    "    accuracy=0\n",
    "    valid_loss = 0        \n",
    "    with th.no_grad():\n",
    "        images = Xvalid\n",
    "        labels = Yvalid\n",
    "        logprobs=model(images)\n",
    "        valid_loss=loss_fn(logprobs,labels)\n",
    "        top_p,top_class=logprobs.topk(1)\n",
    "        accuracy=(top_class.T == labels).sum().float()\n",
    "    train_losses.append(running_loss/nbatch)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accuracies.append(accuracy.item()*100.0/Nvalid)\n",
    "    print(\"Epoch: {}\\t\".format(e+1),\n",
    "              \"train Loss: {:.5f}.. \".format(train_losses[-1]),\n",
    "              \"valid Loss: {:.5f}.. \".format(valid_losses[-1]),\n",
    "              \"valid Accuracy: {:.3f}\".format(valid_accuracies[-1]))  \n",
    "print(\"---------- Best : {:.3f}\".format(max(valid_accuracies)), \" at epoch \" \n",
    "      , np.fromiter(valid_accuracies, dtype=np.float).argmax(), \" / \",epochs )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training loop with a batch size of 1 and then of size 200. The difference in terms of computation time should be significant ! We will now only use a batch size of 200. \n",
    "\n",
    "Note that in practice, the learning rate should be adapted to the mini-batch size. \n",
    "\n",
    "Run the preious training code with a batch size of 200 for 30 epochs. We can plot at the results like this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (9, 1.5))\n",
    "ax= plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax= plt.subplot(1, 3, 2)\n",
    "plt.plot(valid_losses)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax= plt.subplot(1, 3, 3)\n",
    "plt.plot(valid_accuracies)\n",
    "ax.set_ylim(bottom=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training wrapper \n",
    "\n",
    "Write a function that wraps what we need to train a model and look at the results. Test it with a new model. \n",
    "\n",
    "Good to notice : the call of  *model(X)* return a 2D tensor. The 2D tensor has a line for every image of the batch. The line of an image has one column per label (here 10). The tensor contains the log-probabilities that the image corresponds to the label of the\n",
    "column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, epochs=30,verbose=False):\n",
    "    # The verbose allows you to skip printed information per epoch.\n",
    "    # The function will only print the best accuracy on the validation\n",
    "    # and plot the learning curves. \n",
    "    print(\"TODO\")\n",
    "\n",
    "# When we create the model, its parameters are initialized. \n",
    "model=nn.Sequential(nn.Linear(D_in,D_out),\n",
    "                    nn.LogSoftmax(dim=1))\n",
    "optimizer=th.optim.Adam(model.parameters(),lr=0.001)\n",
    "# Note this important to build a new optimizer \n",
    "# if we want to have the reference to parameters \n",
    "# of the new model ! \n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with one hidden layer\n",
    "\n",
    "Now we have a function to train and evaluate the training process of a neural model, we can explore different configurations. Let start with a neural network with one hidden layer and a Sigmoid activation function on this hidden layer. We set the size of this hidden layer to 50. \n",
    "\n",
    "Write the model using the Sequential module, and train it: \n",
    "- for 30 epochs and with lr=0.001 and lr=0.0001\n",
    "- do the same and raise the number of epoch to 50\n",
    "What do you observe ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO \n",
    "hidden_layer = 50 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Sigmoid to ReLU\n",
    "\n",
    "Consider lr=0.0001 and train a similar model with a ReLU activation. Compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of the hidden layer size\n",
    "\n",
    "Run experiments with different hidden layer size, respectively : 50,100,150, 200 and 250. \n",
    "What do you observe ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper network\n",
    "\n",
    "Now we add one more hidden layer and consider a NNet with two hidden layers. \n",
    "The first setup is: \n",
    "- two hidden layers of size 50 with a ReLU activation\n",
    "- a learning rate of 0.0001\n",
    "\n",
    "Train it during 100 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dropout \n",
    "\n",
    "You should observe overfitting, one solution is to add a dropout layer to the model (with a probability of 0.2 for example). Code this modification and rerun the training process to observe the impact. When you use a Dropout layer, the layer acts differently in the train mode and evaluation mode. You should take this into account when you train the model end when you compute the performance on the validation set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different value of dropout to assess its impact on the training process. For example 0.3 and 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can play with different  \"hyper-parameters\":\n",
    "- Increase the size (double for example) of the first hidden layer\n",
    "- Add a third hidden layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "\n",
    "For a good model you obtained, compute the confusion matrix and look at it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA \n",
    "\n",
    "To analyse the results, beyond the confusion matrix, we can compute the PCA (in 2D) and plot the projected datapoints depending on their classes, for instance by considering every pairs of classes. To compute the PCA and project the data, we can use the implementation provided by sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the PCA in 2D\n",
    "from sklearn.decomposition import PCA\n",
    "#  .... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test \n",
    "Take the two best models and run the evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
